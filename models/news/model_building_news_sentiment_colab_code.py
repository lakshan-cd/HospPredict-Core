# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPuPWDZjc1GnSvIjf-gc6jNhofJVGLRX
"""

import pandas as pd
import numpy as np
import gc
from sklearn.metrics import precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.utils import Bunch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from scipy.sparse import hstack, csr_matrix
from imblearn.over_sampling import SMOTE
from google.colab import drive
!pip install transformers
drive.mount('/content/drive')
from google.colab import file
!pip install -U imbalanced-learn
df = pd.read_csv('/content/drive/MyDrive/FYP/NEW/Final_DF_For_Training.csv')
# Identify all hotel price change columns
hotel_columns = [col for col in df.columns if '_change_1d' in col]

# Apply the masking logic
for idx, row in df.iterrows():
    if row['type'] == 'hotel':
        relevant_col = f"{row['hotel']}_change_1d"
        for col in hotel_columns:
            if col != relevant_col:
                df.at[idx, col] = 0.0  # Set non-relevant price changes to 0
# Step 1: Optimize and preprocess
df = df.copy()

df = df.astype({
    'sentiment': 'category',
    'source': 'category',
    'type': 'category',
    'hotel': 'category',
    'hotel_group': 'category',
    'linked_to_stock_spike': 'bool',
    'linked_to_search_spike': 'bool',
    'spike_linked': 'bool',
    'weight': 'float32'
}, errors='ignore')

# Parse date and extract features
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df['day_of_week'] = df['date'].dt.dayofweek.fillna(0).astype('int8')
df['month'] = df['date'].dt.month.fillna(1).astype('int8')
df = df[df['date'].notna()].copy()
df['text'] = df['heading'].fillna('') + ' ' + df['content'].fillna('')
df['sentiment_encoded'] = df['sentiment'].map({'positive': 1, 'neutral': 0, 'negative': -1}).fillna(0).astype('float32')
df['source_encoded'] = df['source'].cat.codes.replace(-1, 0).astype('int8')
df['type_encoded'] = df['type'].cat.codes.replace(-1, 0).astype('int8')
df['hotel_group_encoded'] = df['hotel_group'].cat.codes.replace(-1, 0).astype('int8')

# Convert bools
df['linked_to_search_spike'] = df['linked_to_search_spike'].astype('int8')
df['spike_linked'] = df['spike_linked'].astype('int8')
feature_cols = [
    'sentiment_encoded', 'source_encoded','type_encoded', 'hotel_group_encoded',
    'type_encoded', 'day_of_week', 'month'
]

X_num = df[feature_cols].astype('float32')
y_spike = df['spike_linked']
X_text_raw_train, X_text_raw_test, X_num_train, X_num_test, y_train, y_test = train_test_split(
    df['text'], X_num, y_spike, test_size=0.2, stratify=y_spike, random_state=42
)
vectorizer = TfidfVectorizer(max_features=2000)
X_text_train = vectorizer.fit_transform(X_text_raw_train)
X_text_test = vectorizer.transform(X_text_raw_test)
X_num_train_sparse = csr_matrix(X_num_train.values)
X_num_test_sparse = csr_matrix(X_num_test.values)
X_train_combined = hstack([X_text_train, X_num_train_sparse], format='csr')
X_test_combined = hstack([X_text_test, X_num_test_sparse], format='csr')
smote = SMOTE(random_state=42)
X_train_dense = X_train_combined.toarray()  # if memory allows
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_dense, y_train)
print("Before SMOTE:")
print(pd.Series(y_train).value_counts())
print("After SMOTE:")
print(pd.Series(y_train_balanced).value_counts())
spike_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
spike_model.fit(X_train_balanced, y_train_balanced)

# --- Step 6.1: Evaluate ---
y_pred = spike_model.predict(X_test_combined)
print("📊 Spike Prediction")
print(classification_report(y_test, y_pred))
import pandas as pd

# Combine features and labels for easier sampling
X_test_df = pd.DataFrame(X_test_combined.toarray())  # if sparse
y_test_df = pd.Series(y_test).reset_index(drop=True)
test_df = pd.concat([X_test_df, y_test_df.rename('label')], axis=1)

# Separate by class
class_0 = test_df[test_df['label'] == 0]
class_1 = test_df[test_df['label'] == 1]

# Undersample class 0 to match class 1
n_samples = len(class_1)
class_0_sampled = class_0.sample(n=n_samples, random_state=42)

# Combine to form balanced test set
test_balanced = pd.concat([class_0_sampled, class_1], axis=0).sample(frac=1, random_state=42)

# Split X and y again
X_test_balanced = test_balanced.drop(columns=['label']).values
y_test_balanced = test_balanced['label'].values
y_pred_balanced = spike_model.predict(X_test_balanced)

from sklearn.metrics import classification_report
print(classification_report(y_test_balanced, y_pred_balanced))
import joblib

# Create path
drive_path = '/content/drive/MyDrive/FYP/Models/'
spike_model_path = drive_path + 'spike_prediction_model.pkl'
vectorizer_path = drive_path + 'spike_vectorizer.pkl'

# Save to Drive
joblib.dump(spike_model, spike_model_path)
joblib.dump(vectorizer, vectorizer_path)

# Predict
y_pred = spike_model.predict(X_test_combined)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)
labels = ['No Spike (0)', 'Spike (1)']

# Pretty print confusion matrix
print("🔍 Confusion Matrix (Raw Counts):")
print(pd.DataFrame(cm, index=[f'True {l}' for l in labels],
                      columns=[f'Pred {l}' for l in labels]))

# Optional: Plot heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("📊 Confusion Matrix")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.show()
# Get predicted probabilities
y_proba = spike_model.predict_proba(X_test_combined)

# For binary classification, probabilities for class 1 (spike)
spike_confidence = y_proba[:, 1]

# Build a DataFrame to compare
import pandas as pd

comparison_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred,
    'Confidence': spike_confidence
})

# Optionally sort by confidence
comparison_df = comparison_df.sort_values(by='Confidence', ascending=False)

# Print a few rows (e.g., top 10 confident predictions)
print(comparison_df.head(20))
def create_price_target(change, threshold=0.005):
    if pd.isna(change):
        return np.nan
    if change > threshold:
        return 'up'
    elif change < -threshold:
        return 'down'
    else:
        return 'neutral'

hotel_columns = [col for col in df.columns if '_change_1d' in col]
price_models = {}
metrics_summary = []
hotel_columns = [col for col in df.columns if '_change_1d' in col]

for col in hotel_columns:
    hotel = col.replace('_change_1d', '')
    df[f'{hotel}_target'] = df[col].astype('float32', errors='ignore').apply(create_price_target)
    # Filter valid data
    y_price = df[f'{hotel}_target']
    valid_mask = y_price.notna()

    if valid_mask.sum() < 20:
        print(f"Skipping {hotel} — not enoughsamples.")
        continue

    X_price_text_raw = df.loc[valid_mask, 'text']
    X_price_text = vectorizer.transform(X_price_text_raw)
    X_price_num = csr_matrix(df.loc[valid_mask, feature_cols].astype('float32').values)
    X_price_combined = hstack([X_price_text, X_price_num], format='csr')

    y_price = y_price[valid_mask]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X_price_combined, y_price, test_size=0.2, stratify=y_price, random_state=42
    )

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    price_models[hotel] = model

    # Evaluate
    y_pred = model.predict(X_test)
    print(f"\n📊 Price Change Prediction for {hotel}")
    print(classification_report(y_test, y_pred))
    from sklearn.metrics import precision_recall_fscore_support

    # After model evaluation
    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro', zero_division=0)
    metrics_summary.append({
        'hotel': hotel,
        'precision': prec,
        'recall': rec,
        'f1_score': f1,
        'samples': len(y_test)
    })